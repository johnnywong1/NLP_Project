{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65992955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.10.2-cp36-cp36m-manylinux2010_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.2\n",
      "    Uninstalling tokenizers-0.10.2:\n",
      "      Successfully uninstalled tokenizers-0.10.2\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Successfully installed tokenizers-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eb048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: ray in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (5.3.1)\n",
      "Requirement already satisfied: aiohttp-cors in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.7.0)\n",
      "Requirement already satisfied: filelock in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (3.0.12)\n",
      "Requirement already satisfied: requests in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (2.25.1)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.10.1)\n",
      "Requirement already satisfied: aioredis in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: colorama in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.4.4)\n",
      "Requirement already satisfied: opencensus in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.7.12)\n",
      "Requirement already satisfied: gpustat in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (1.0.2)\n",
      "Requirement already satisfied: jsonschema in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (3.2.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (3.15.8)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (1.37.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (1.19.5)\n",
      "Requirement already satisfied: aiohttp in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (3.7.4.post0)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (0.3.5)\n",
      "Requirement already satisfied: redis>=3.5.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (3.5.3)\n",
      "Requirement already satisfied: dataclasses in /u/kem5en/.local/lib/python3.6/site-packages (from ray) (0.8)\n",
      "Requirement already satisfied: click>=7.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from ray) (7.1.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (3.7.4.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (20.3.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from aiohttp->ray) (4.0.0)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /u/kem5en/.local/lib/python3.6/site-packages (from aiohttp->ray) (1.6.3)\n",
      "Requirement already satisfied: idna>=2.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from idna-ssl>=1.0->aiohttp->ray) (2.10)\n",
      "Requirement already satisfied: hiredis in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from aioredis->ray) (1.1.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from gpustat->ray) (7.352.0)\n",
      "Requirement already satisfied: psutil in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from gpustat->ray) (5.8.0)\n",
      "Requirement already satisfied: blessings>=1.6 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from gpustat->ray) (1.7)\n",
      "Requirement already satisfied: setuptools in /u/kem5en/.local/lib/python3.6/site-packages (from jsonschema->ray) (56.0.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from jsonschema->ray) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in /u/kem5en/.local/lib/python3.6/site-packages (from jsonschema->ray) (3.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /u/kem5en/.local/lib/python3.6/site-packages (from importlib-metadata->jsonschema->ray) (3.4.1)\n",
      "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from opencensus->ray) (1.26.3)\n",
      "Requirement already satisfied: opencensus-context==0.1.2 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from opencensus->ray) (0.1.2)\n",
      "Requirement already satisfied: contextvars in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from opencensus-context==0.1.2->opencensus->ray) (2.4)\n",
      "Requirement already satisfied: pytz in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2021.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (20.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.53.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /u/kem5en/.local/lib/python3.6/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.29.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /u/kem5en/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /u/kem5en/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /u/kem5en/.local/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /u/kem5en/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->ray) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->ray) (1.26.4)\n",
      "Requirement already satisfied: immutables>=0.9 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from contextvars->opencensus-context==0.1.2->opencensus->ray) (0.15)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cf5fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "#!pip install pytorch-lightning transformers keras matplotlib numpy sklearn nltk pytorch-pretrained-bert pytorch-nlp matplotlib sklearn keras\n",
    "#Lightning Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import ray.tune\n",
    "\n",
    "#Other Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification\n",
    "#from transformers import BerttokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pickle\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "class ProjData(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str = \"~/tmp\", batch_size: int = 32, max_len : int = 128, ratio : int = 2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # self.batch_size = batch_size\n",
    "        self.max_len = max_len # Bert Max Len input\n",
    "        self.rat = ratio\n",
    "        self.name = \"a-\" + str(max_len) + \"-\" + str(ratio)\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        # *** tokenizer isn't actually a constant and the do_lower_case should be redundant if preprocessing was correct.\n",
    "        # self.tokenizer = BerttokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True, do_lower_case=True)\n",
    "\n",
    "\n",
    "        ###\n",
    "        # Preprocessing Data\n",
    "        ###\n",
    "        print(\"Getting pos\")\n",
    "        pos_data = self.get_pos_dataset()\n",
    "        print(\"Getting neg\")\n",
    "        neg_data = self.get_neg_dataset()\n",
    "\n",
    "        # *** TODO: Proper subset selection either in concat_datasets or in get_neg_dataset\n",
    "        print(\"Joining\")\n",
    "        dataset = self.concat_datasets(pos_data, neg_data)\n",
    "        dataset.dropna(inplace=True) # Losing negative examples potentially...\n",
    "        dataset[\"data\"] = dataset[\"data\"].map(self.preprocess)\n",
    "\n",
    "        # 60% - train set, 20% - validation set, 20% - test set\n",
    "        train, validate, test = np.split(dataset.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "\n",
    "        X_train, y_train = train[\"data\"], train[\"label\"]\n",
    "        X_val, y_val = validate[\"data\"], validate[\"label\"]\n",
    "        X_test, y_test = test[\"data\"], test[\"label\"]    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # NOTE: This is a small subset used for testing... likely will remove in final ver.\n",
    "        # *** Set to \"None\" to skip - if there any reason you take the tails of the lists rather than the heads?\n",
    "        PROTOTYPE_NUM = None\n",
    "        # **** NEED TO SHUFFLE FIRST IF YOU USE THIS\n",
    "        if PROTOTYPE_NUM:\n",
    "            X_train = X_train[:PROTOTYPE_NUM * 10]\n",
    "            y_train = y_train[:PROTOTYPE_NUM * 10]\n",
    "            X_val = X_val[:PROTOTYPE_NUM]\n",
    "            y_val = y_val[:PROTOTYPE_NUM]\n",
    "            X_test = X_test[:PROTOTYPE_NUM]\n",
    "            y_test = y_test[:PROTOTYPE_NUM]\n",
    "\n",
    "        ###\n",
    "        # Tokenization\n",
    "        ###\n",
    "        # Convert texts into tokens. (These are not truncated or padded yet)\n",
    "        print(\"Tokenizing\")\n",
    "        pre_train_input_ids = self.tokenize_datasets(X_train, self.tokenizer)\n",
    "        pre_val_input_ids = self.tokenize_datasets(X_val, self.tokenizer)\n",
    "        pre_test_input_ids = self.tokenize_datasets(X_test, self.tokenizer)\n",
    "        \n",
    "        # Truncate and Pad your tokens\n",
    "        print(\"Padding\")\n",
    "        train_input_ids = self.trunc_n_pad(pre_train_input_ids)\n",
    "        val_input_ids = self.trunc_n_pad(pre_val_input_ids)\n",
    "        test_input_ids = self.trunc_n_pad(pre_test_input_ids)\n",
    "\n",
    "        ###\n",
    "        # Misc.\n",
    "        ###\n",
    "        # Create attention masks\n",
    "        print(\"Creating masks\")\n",
    "        train_attention_masks = self.create_attention_masks(train_input_ids)\n",
    "        val_attention_masks = self.create_attention_masks(val_input_ids)\n",
    "        test_attention_masks = self.create_attention_masks(test_input_ids)\n",
    "\n",
    "        # # Convert all of our data into torch tensors, the required datatype for our model\n",
    "        train_inputs = torch.tensor(train_input_ids)\n",
    "        validation_inputs = torch.tensor(val_input_ids)\n",
    "\n",
    "        train_labels = torch.tensor(y_train.values.tolist())\n",
    "        validation_labels = torch.tensor(y_val.values.tolist())\n",
    "\n",
    "        train_masks = torch.tensor(train_attention_masks)\n",
    "        validation_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "        test_inputs = torch.tensor(test_input_ids)\n",
    "        test_labels = torch.tensor(y_test.values.tolist())\n",
    "\n",
    "        test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "        # Create an iterator of our data with torch DataLoader. \n",
    "        self.train = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        self.val = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        self.test = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "        print(train_inputs)\n",
    "        print(self.train)\n",
    "\n",
    "    def process(self, sent):\n",
    "        s = self.preprocess(sent)\n",
    "        s = self.tokenize_datasets([s], self.tokenizer)\n",
    "        s = self.trunc_n_pad(s)\n",
    "        mask = self.create_attention_masks(s)\n",
    "        return  torch.tensor(s),  torch.tensor(mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, x):\n",
    "\n",
    "        # self.re1 = re.compile('\\n') \n",
    "        # self.re2 = re.compile('[\\w]+')\n",
    "        x = re.sub(r'(\\n+)',r' ', x)\n",
    "        # lower case in tokenizer\n",
    "        x = \" \".join(re.findall('[\\w]+',x))\n",
    "        return x\n",
    "        \n",
    "        # # RNV uses a special preprocess step --- mirror it for pushio.\n",
    "        # # print(\"Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\")\n",
    "        # ### 1) remove newlines\n",
    "        # data['data'] = data['data'].replace('\\n', ' ', regex = True)\n",
    "\n",
    "        # ## 2) convert to lowercase\n",
    "        # data['data'] = data['data'].str.lower()\n",
    "\n",
    "        # # ### 3) remove punct and numbers: https://stackoverflow.com/questions/47947438/preprocessing-string-data-in-pandas-dataframe\n",
    "        # data[\"data\"] = data.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    " \n",
    "\n",
    "    # Import PushIO CSV    \n",
    "    def get_neg_dataset(self, path=\"/bigtemp/rm5tx/nlp_project/2016-05_all.csv\"):\n",
    "        # path = \"/localtmp/rm5tx/nlp_project/2016-05_all.csv\"\n",
    "        print(\"reading neg\")\n",
    "        data = pd.read_csv(path, usecols=['body'], dtype=\"string\")\n",
    "        data.rename(columns={\"body\":\"data\"}, inplace=True)\n",
    "        # print(\"filtering n/a neg\")\n",
    "        # data.dropna(inplace=True)\n",
    "        # print(\"preprocessing neg\") --- less work to do this on the joined\n",
    "        # data[\"data\"] = data[\"data\"].map(self.preprocess)\n",
    "        data[\"label\"] = 0\n",
    "        # We want a unify col name for when we concat pos and neg data\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Reddit Norm Violations\n",
    "    def get_pos_dataset(self, path=\"/bigtemp/rm5tx/nlp_project/reddit-norm-violations/data/macro-norm-violations/\"):\n",
    "        directory = os.path.abspath(path)\n",
    "\n",
    "        pos_temp = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                with open(root+ \"/\" +file) as f:\n",
    "                    pos_temp += f.readlines()\n",
    "        data = pd.DataFrame(data=pos_temp, dtype = \"string\")\n",
    "        data.rename(columns={0:\"data\"}, inplace=True)\n",
    "        data[\"label\"] = 1\n",
    "        return data\n",
    "\n",
    "    def get_adjacents_datasets(self):\n",
    "        neg_init = self.get_neg_dataset()\n",
    "        pos_init = self.get_neg_dataset()\n",
    "        pos, neg = 0, 0\n",
    "        return pos, neg\n",
    "\n",
    "    def concat_datasets(self, data_a, data_b):\n",
    "        if len(data_a.index) > len(data_b.index):\n",
    "            data_a, data_b = data_b, data_a\n",
    "        # frames = [data_a, data_b[(self.rat * len(data_a.index)):]]\n",
    "        trunced_b = data_b.sample(n=int(self.rat * len(data_a.index)), random_state=42)\n",
    "        print(\"Using \", len(data_a.index), len(trunced_b.index), \" samples.\")\n",
    "        dataset = pd.concat([data_a, trunced_b])\n",
    "        print(\"Total = \", len(dataset.index))\n",
    "        return dataset\n",
    "\n",
    "    ###Pre-processing Code###\n",
    "    def tokenize_datasets(self, X_dataset, tokenizer):\n",
    "        input_ids = []\n",
    "        for sent in X_dataset:\n",
    "            tokenized_text = tokenizer.encode(\n",
    "                                            sent,                      # Sentence to encode\n",
    "                                            add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens\n",
    "                                            max_length = self.max_len,      # Truncate senences\n",
    "                                            truncation=True,\n",
    "                                            )\n",
    "            input_ids.append(tokenized_text)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "    # Appears that CS Serv don[t have tf version 2.2]\n",
    "    # Thus, we cannot use the convenient pad_sequences from keras\n",
    "    def trunc_n_pad(self, input_id_list):\n",
    "        ret_list = []\n",
    "        for input_id in input_id_list:\n",
    "            if len(input_id) > self.max_len:\n",
    "                ret_list.append(input_id[:self.max_len])\n",
    "            elif len(input_id) < self.max_len:\n",
    "                temp_sublist = input_id + [0] * (self.max_len - len(input_id))\n",
    "                ret_list.append(temp_sublist)\n",
    "            else:\n",
    "                ret_list.append(input_id)\n",
    "        return ret_list\n",
    "\n",
    "    # Create attention masks\n",
    "    def create_attention_masks(self, input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            # Create a mask of 1s for each token followed by 0s for padding\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def train_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.val, batch_size=batch_size)\n",
    "\n",
    "    def test_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.test, batch_size=sbatch_size)\n",
    "\n",
    "    def save(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        path = \"/bigtemp/jw6qs/\"\n",
    "\n",
    "        torch.save(self.tokenizer, open(path+self.name+\"token.p\", \"wb\"))\n",
    "        torch.save(self.train, open(path+self.name+\"train.pt\", \"wb\"))\n",
    "        torch.save(self.val, open(path+self.name+\"val.pt\", \"wb\"))\n",
    "        torch.save(self.test, open(path+self.name+\"test.pt\", \"wb\"))\n",
    "\n",
    "    def load(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        path = \"/bigtemp/jw6qs/\"\n",
    "\n",
    "        self.tokenizer = torch.load(open(path+self.name+\"token.p\", \"rb\"))\n",
    "        self.train = torch.load(open(path+self.name+\"train.pt\", \"rb\"))\n",
    "        self.val = torch.load(open(path+self.name+\"val.pt\", \"rb\"))\n",
    "        self.test = torch.load(open(path+self.name+\"test.pt\", \"rb\"))    \n",
    "\n",
    "class ProjModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # model_name_or_path: str,\n",
    "        num_labels: int = 2,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-9,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        # eval_splits: Optional[list] = None,\n",
    "        **kwargs\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "        # self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        # self.metric = ...\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        logits = self.model(x, attention_mask=mask).logits\n",
    "        # return logits        \n",
    "        pred = torch.argmax(logits, 1)\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        out = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = out.logits\n",
    "        loss = out.loss\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('train_acc', acc)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # *** Old Monitor, NYI -- Do we want to use a scheduler at all?\n",
    "    # https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\n",
    "    #     # for batch in validation_dataloader:\n",
    "    #     #     with torch.no_grad():\n",
    "    #     #         logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0] \n",
    "\n",
    "    #     #     tmp_eval_nb = accurate_nb(logits, b_labels)\n",
    "\n",
    "    #     #     eval_accurate_nb += tmp_eval_nb\n",
    "    #     #     nb_eval_examples += label_ids.shape[0]\n",
    "    #     # eval_accuracy = eval_accurate_nb/nb_eval_examples\n",
    "    #     # print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "    #     # scheduler.step(eval_accuracy)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask).logits\n",
    "        \n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('valid_acc', acc)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask).logits\n",
    "\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('test_acc', acc)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ###\n",
    "        # Param Optim.\n",
    "        ###\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': self.hparams.weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "        # return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'eval_acc'}\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def accurate_nb(self, logits, labels):\n",
    "        pred_flat = torch.argmax(logits, dim=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return torch.sum(pred_flat == labels_flat) / labels_flat.shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd0c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 32\n",
    "    TEST_BATCH_SIZE = 32\n",
    "\n",
    "    #LEARNING_RATE = 5e-5\n",
    "    #MAX_EPOCHS = 2\n",
    "    WEIGHT_DECAY = 0.2\n",
    "    \n",
    "    config = {\n",
    "        \"LEARNING_RATE\": ray.tune.loguniform(.005, 5e-5),\n",
    "        #\"BATCH_SIZE\": tune.choice([32,64,128]),\n",
    "        \"MAX_EPOCHS\": ray.tune.choice([2, 5, 8])\n",
    "    }\n",
    "    \n",
    "    metrics = {\"loss\": \"ptl/val_loss\", \"acc\": \"ptl/val_accuracy\"}\n",
    "    \n",
    "    callback = [TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "        \n",
    "    SEED = 7\n",
    "    # DATA_PATH = \"/bigtemp/rm5tx/nlp_project/data_cache/\"\n",
    "    DATA_PATH = \"/localtmp/rm5tx/nlp_project/data_cache/\"\n",
    "    \n",
    "    data = ProjData(max_len=128, ratio=2)\n",
    "    try:\n",
    "        data.load(DATA_PATH)\n",
    "        print(\"Loaded Saved Data\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        data.setup()\n",
    "        data.save(DATA_PATH)\n",
    "    ### Comment out the try block and uncomment below while you're working on the data part or you'll just skip it and use old data.\n",
    "    # data.setup() \n",
    "    # data.save(DATA_PATH)\n",
    "    \n",
    "    model = ProjModel(learning_rate = config[\"LEARNING_RATE\"], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"nlp_proj\")\n",
    "   \n",
    "    def train_tune(config):\n",
    "        trainer = pl.Trainer(logger=logger, \n",
    "                                accelerator='dp', # jupyter can't use ddp, use dp instead\n",
    "                                # effective batch size is batch_size * num_gpus * num_nodes\n",
    "                                gpus=1, \n",
    "                                gradient_clip_val=1.0, \n",
    "                                fast_dev_run=False,\n",
    "                                progress_bar_refresh_rate=0,\n",
    "                                callbacks= callbacks\n",
    "                            )\n",
    "        trainer.fit(model, data.train_dataloader(batch_size=TRAIN_BATCH_SIZE), \n",
    "                        data.val_dataloader(batch_size=VAL_BATCH_SIZE))\n",
    "    \n",
    "    from functools import partial\n",
    "    from ray.tune import CLIReporter\n",
    "    \n",
    "    reporter = CLIReporter(\n",
    "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    analysis = ray.tune.run(\n",
    "        ray.tune.with_parameters(train_tune, epochs = 10),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": 2},\n",
    "        config = config,\n",
    "        num_samples = 5,\n",
    "        progress_reporter = reporter\n",
    "    )\n",
    "    print(analysis.best_config)\n",
    "    \n",
    "    model.eval()\n",
    "    sents = [\"random sentence\", \"pretty flowers\", \"idiot\", \"fuck you cunt nigger\"]\n",
    "    for sent in sents:\n",
    "        x, mask = data.process(sent)\n",
    "        print(model(x, mask))\n",
    "                      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7520c81d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Saved Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2021-05-04 22:19:04,463\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-05-04 22:19:08,384\tWARNING function_runner.py:545 -- Function checkpointing is disabled. This may result in unexpected behavior when using checkpointing features or certain schedulers. To enable, set the train function arguments to be `func(config, checkpoint_dir=None)`.\n",
      "2021-05-04 22:19:29,755\tWARNING worker.py:1115 -- Warning: The actor ImplicitFunc has size 457059008 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Memory usage on this node: 42.8/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/40 CPUs, 2.0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00000 | RUNNING  |       |     9.19498e-05 |            5 |\n",
      "| train_tune_477e4_00001 | PENDING  |       |     0.000373248 |            8 |\n",
      "| train_tune_477e4_00002 | PENDING  |       |     0.00360995  |            2 |\n",
      "| train_tune_477e4_00003 | PENDING  |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | PENDING  |       |     0.000357641 |            5 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-04 22:19:35,069\tWARNING worker.py:1115 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID ebeb91f64e8344cea8b3a1e301000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.trainer.trainer import Trainer\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "    from pytorch_lightning.loggers import LightningLoggerBase\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "    raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m 2021-05-04 22:19:34,860\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.trainer.trainer import Trainer\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.loggers import LightningLoggerBase\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     from torch.utils.tensorboard import SummaryWriter\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m 2021-05-04 22:19:34,872\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43838, ip=128.143.67.29)\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m     raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "\u001b[2m\u001b[36m(pid=43838)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "2021-05-04 22:19:37,259\tERROR trial_runner.py:732 -- Trial train_tune_477e4_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/worker.py\", line 1483, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43838, ip=128.143.67.29)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "    raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_477e4_00000:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 45.0/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/40 CPUs, 2.0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (1 ERROR, 3 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00001 | RUNNING  |       |     0.000373248 |            8 |\n",
      "| train_tune_477e4_00002 | PENDING  |       |     0.00360995  |            2 |\n",
      "| train_tune_477e4_00003 | PENDING  |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | PENDING  |       |     0.000357641 |            5 |\n",
      "| train_tune_477e4_00000 | ERROR    |       |     9.19498e-05 |            5 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "Number of errored trials: 1\n",
      "+------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                       |\n",
      "|------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_477e4_00000 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00000_0_LEARNING_RATE=9.195e-05,MAX_EPOCHS=5_2021-05-04_22-19-25/error.txt |\n",
      "+------------------------+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-04 22:19:38,527\tWARNING worker.py:1115 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID f7337c6e6a130fc94ea16ab001000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.trainer.trainer import Trainer\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "    from pytorch_lightning.loggers import LightningLoggerBase\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "    raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m 2021-05-04 22:19:38,515\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.trainer.trainer import Trainer\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.loggers import LightningLoggerBase\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     from torch.utils.tensorboard import SummaryWriter\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m 2021-05-04 22:19:38,555\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43830, ip=128.143.67.29)\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m     raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "\u001b[2m\u001b[36m(pid=43830)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "2021-05-04 22:19:44,062\tERROR trial_runner.py:732 -- Trial train_tune_477e4_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/worker.py\", line 1483, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43830, ip=128.143.67.29)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "    raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_477e4_00001:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.9/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/40 CPUs, 2.0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (2 ERROR, 2 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00002 | RUNNING  |       |     0.00360995  |            2 |\n",
      "| train_tune_477e4_00003 | PENDING  |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | PENDING  |       |     0.000357641 |            5 |\n",
      "| train_tune_477e4_00000 | ERROR    |       |     9.19498e-05 |            5 |\n",
      "| train_tune_477e4_00001 | ERROR    |       |     0.000373248 |            8 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "Number of errored trials: 2\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                        |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_477e4_00000 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00000_0_LEARNING_RATE=9.195e-05,MAX_EPOCHS=5_2021-05-04_22-19-25/error.txt  |\n",
      "| train_tune_477e4_00001 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00001_1_LEARNING_RATE=0.00037325,MAX_EPOCHS=8_2021-05-04_22-19-30/error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-04 22:19:45,925\tWARNING worker.py:1115 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID 3d143689cd1b886148e2e08e01000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.trainer.trainer import Trainer\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "    from pytorch_lightning.loggers import LightningLoggerBase\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "    raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m 2021-05-04 22:19:45,690\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.trainer.trainer import Trainer\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.loggers import LightningLoggerBase\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     from torch.utils.tensorboard import SummaryWriter\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m 2021-05-04 22:19:45,735\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43833, ip=128.143.67.29)\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m     raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "\u001b[2m\u001b[36m(pid=43833)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "\n",
      "2021-05-04 22:19:48,806\tWARNING util.py:162 -- The `start_trial` operation took 4.733 s, which may be a performance bottleneck.\n",
      "2021-05-04 22:19:51,783\tWARNING util.py:162 -- The `start_trial` operation took 2.975 s, which may be a performance bottleneck.\n",
      "2021-05-04 22:19:51,789\tERROR trial_runner.py:732 -- Trial train_tune_477e4_00002: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/worker.py\", line 1483, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43833, ip=128.143.67.29)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "    raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_477e4_00002:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 44.3/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 2.0/40 CPUs, 2.0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (3 ERROR, 1 PENDING, 1 RUNNING)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00003 | RUNNING  |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | PENDING  |       |     0.000357641 |            5 |\n",
      "| train_tune_477e4_00000 | ERROR    |       |     9.19498e-05 |            5 |\n",
      "| train_tune_477e4_00001 | ERROR    |       |     0.000373248 |            8 |\n",
      "| train_tune_477e4_00002 | ERROR    |       |     0.00360995  |            2 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "Number of errored trials: 3\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                        |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_477e4_00000 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00000_0_LEARNING_RATE=9.195e-05,MAX_EPOCHS=5_2021-05-04_22-19-25/error.txt  |\n",
      "| train_tune_477e4_00001 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00001_1_LEARNING_RATE=0.00037325,MAX_EPOCHS=8_2021-05-04_22-19-30/error.txt |\n",
      "| train_tune_477e4_00002 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00002_2_LEARNING_RATE=0.0036099,MAX_EPOCHS=2_2021-05-04_22-19-34/error.txt  |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m 2021-05-04 22:19:52,825\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.trainer.trainer import Trainer\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.loggers import LightningLoggerBase\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     from torch.utils.tensorboard import SummaryWriter\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m 2021-05-04 22:19:52,836\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43821, ip=128.143.67.29)\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m     raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "\u001b[2m\u001b[36m(pid=43821)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "2021-05-04 22:19:52,919\tWARNING worker.py:1115 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID 4911a8d91ac882d25f235a6a01000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.trainer.trainer import Trainer\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "    from pytorch_lightning.loggers import LightningLoggerBase\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "    raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\n",
      "2021-05-04 22:19:56,767\tWARNING util.py:162 -- The `start_trial` operation took 4.967 s, which may be a performance bottleneck.\n",
      "2021-05-04 22:19:56,785\tERROR trial_runner.py:732 -- Trial train_tune_477e4_00003: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/worker.py\", line 1483, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43821, ip=128.143.67.29)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "    raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_477e4_00003:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-04 22:20:00,056\tWARNING worker.py:1115 -- Failed to unpickle actor class 'ImplicitFunc' for actor ID 39e721bd90770eec946c999201000000. Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "    actor_class = pickle.loads(pickled_class)\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "    from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "    from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.trainer.trainer import Trainer\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "    from pytorch_lightning.loggers import LightningLoggerBase\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "    from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "    from torch.utils.tensorboard import SummaryWriter\n",
      "  File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "    raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\n",
      "2021-05-04 22:20:00,076\tERROR trial_runner.py:732 -- Trial train_tune_477e4_00004: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 702, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 686, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 47, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/worker.py\", line 1483, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43836, ip=128.143.67.29)\n",
      "  File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "    raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m 2021-05-04 22:20:00,047\tERROR function_manager.py:498 -- Failed to load actor class ImplicitFunc.\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 496, in _load_actor_class_from_gcs\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     actor_class = pickle.loads(pickled_class)\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/__init__.py\", line 29, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/__init__.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.callbacks.swa import StochasticWeightAveraging\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/callbacks/swa.py\", line 26, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.trainer.optimizers import _get_default_scheduler_config\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.trainer.trainer import Trainer\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\", line 30, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.loggers import LightningLoggerBase\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/__init__.py\", line 18, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py\", line 25, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     from torch.utils.tensorboard import SummaryWriter\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.local/lib/python3.6/site-packages/torch/utils/tensorboard/__init__.py\", line 4, in <module>\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     raise ImportError('TensorBoard logging requires TensorBoard version 1.15 or above')\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m ImportError: TensorBoard logging requires TensorBoard version 1.15 or above\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m 2021-05-04 22:20:00,071\tERROR worker.py:382 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::ImplicitFunc.__init__()\u001b[39m (pid=43836, ip=128.143.67.29)\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 505, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"python/ray/_raylet.pyx\", line 449, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 556, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m   File \"/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 461, in temporary_actor_method\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m     raise RuntimeError(f\"The actor with name {actor_class_name} \"\n",
      "\u001b[2m\u001b[36m(pid=43836)\u001b[0m RuntimeError: The actor with name ImplicitFunc failed to be imported, and so cannot execute this method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_tune_477e4_00004:\n",
      "  {}\n",
      "  \n",
      "== Status ==\n",
      "Memory usage on this node: 43.6/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/40 CPUs, 0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (5 ERROR)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00000 | ERROR    |       |     9.19498e-05 |            5 |\n",
      "| train_tune_477e4_00001 | ERROR    |       |     0.000373248 |            8 |\n",
      "| train_tune_477e4_00002 | ERROR    |       |     0.00360995  |            2 |\n",
      "| train_tune_477e4_00003 | ERROR    |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | ERROR    |       |     0.000357641 |            5 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "Number of errored trials: 5\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                        |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_477e4_00000 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00000_0_LEARNING_RATE=9.195e-05,MAX_EPOCHS=5_2021-05-04_22-19-25/error.txt  |\n",
      "| train_tune_477e4_00001 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00001_1_LEARNING_RATE=0.00037325,MAX_EPOCHS=8_2021-05-04_22-19-30/error.txt |\n",
      "| train_tune_477e4_00002 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00002_2_LEARNING_RATE=0.0036099,MAX_EPOCHS=2_2021-05-04_22-19-34/error.txt  |\n",
      "| train_tune_477e4_00003 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00003_3_LEARNING_RATE=0.00019451,MAX_EPOCHS=5_2021-05-04_22-19-41/error.txt |\n",
      "| train_tune_477e4_00004 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00004_4_LEARNING_RATE=0.00035764,MAX_EPOCHS=5_2021-05-04_22-19-48/error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Memory usage on this node: 43.6/503.1 GiB\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/40 CPUs, 0/4 GPUs, 0.0/317.5 GiB heap, 0.0/140.06 GiB objects (0.0/1.0 accelerator_type:RTX)\n",
      "Result logdir: /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09\n",
      "Number of trials: 5/5 (5 ERROR)\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "| Trial name             | status   | loc   |   LEARNING_RATE |   MAX_EPOCHS |\n",
      "|------------------------+----------+-------+-----------------+--------------|\n",
      "| train_tune_477e4_00000 | ERROR    |       |     9.19498e-05 |            5 |\n",
      "| train_tune_477e4_00001 | ERROR    |       |     0.000373248 |            8 |\n",
      "| train_tune_477e4_00002 | ERROR    |       |     0.00360995  |            2 |\n",
      "| train_tune_477e4_00003 | ERROR    |       |     0.000194505 |            5 |\n",
      "| train_tune_477e4_00004 | ERROR    |       |     0.000357641 |            5 |\n",
      "+------------------------+----------+-------+-----------------+--------------+\n",
      "Number of errored trials: 5\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name             |   # failures | error file                                                                                                                                        |\n",
      "|------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_tune_477e4_00000 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00000_0_LEARNING_RATE=9.195e-05,MAX_EPOCHS=5_2021-05-04_22-19-25/error.txt  |\n",
      "| train_tune_477e4_00001 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00001_1_LEARNING_RATE=0.00037325,MAX_EPOCHS=8_2021-05-04_22-19-30/error.txt |\n",
      "| train_tune_477e4_00002 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00002_2_LEARNING_RATE=0.0036099,MAX_EPOCHS=2_2021-05-04_22-19-34/error.txt  |\n",
      "| train_tune_477e4_00003 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00003_3_LEARNING_RATE=0.00019451,MAX_EPOCHS=5_2021-05-04_22-19-41/error.txt |\n",
      "| train_tune_477e4_00004 |            1 | /u/kem5en/ray_results/train_tune_2021-05-04_22-19-09/train_tune_477e4_00004_4_LEARNING_RATE=0.00035764,MAX_EPOCHS=5_2021-05-04_22-19-48/error.txt |\n",
      "+------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [train_tune_477e4_00000, train_tune_477e4_00001, train_tune_477e4_00002, train_tune_477e4_00003, train_tune_477e4_00004])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5b52d35e4541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6708525ce0ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mprogress_reporter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreporter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [train_tune_477e4_00000, train_tune_477e4_00001, train_tune_477e4_00002, train_tune_477e4_00003, train_tune_477e4_00004])"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
