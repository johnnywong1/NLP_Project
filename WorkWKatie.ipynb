{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    !pip install matplotlib transformers numpy torch sklearn nltk pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-fiber",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#If there's a GPU available...\n",
    "if torch.cuda.is_available():        \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emerging-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-tourist",
   "metadata": {},
   "source": [
    "# Pre-processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intense-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import PushIO CSV\n",
    "neg_data = pd.read_csv(\"/bigtemp/rm5tx/nlp_project/2016-05_all.csv\", usecols=['body'], dtype=\"string\")\n",
    "neg_data.rename(columns={\"body\":\"data\"}, inplace=True)\n",
    "neg_data[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blessed-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit Norm Violations\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pos_temp = []\n",
    "directory = os.path.abspath(\"/bigtemp/rm5tx/nlp_project/reddit-norm-violations/data/macro-norm-violations/\")\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "        with open(root+ \"/\" +file) as f:\n",
    "            pos_temp += f.readlines()\n",
    "pos_data = pd.DataFrame(data=pos_temp, dtype = \"string\")\n",
    "pos_data.rename(columns={0:\"data\"}, inplace=True)\n",
    "pos_data[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thorough-japanese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\n"
     ]
    }
   ],
   "source": [
    "###preprocessing -\n",
    "print(\"Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\")\n",
    "\n",
    "### 1) remove newlines\n",
    "pos_data['data'] = pos_data['data'].replace('\\n', ' ', regex = True)\n",
    "\n",
    "## 2) convert to lowercase\n",
    "pos_data['data'] = pos_data['data'].str.lower()\n",
    "\n",
    "# ### 3) remove punct and numbers: https://stackoverflow.com/questions/47947438/preprocessing-string-data-in-pandas-dataframe\n",
    "import re\n",
    "pos_data[\"data\"] = pos_data.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corresponding-logan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thats 1 case per 5000 people. nice!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What's the difference between roast beef and p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ya, you got tuh stick it to da man!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  label\n",
       "0                thats 1 case per 5000 people. nice!      0\n",
       "1  What's the difference between roast beef and p...      0\n",
       "2                Ya, you got tuh stick it to da man!      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [neg_data, pos_data]\n",
    "dataset = pd.concat(frames)\n",
    "dataset.dropna(inplace=True)\n",
    "# print(len(neg_data) + len(pos_data) == len(data))\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "funded-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60% - train set, 20% - validation set, 20% - test set\n",
    "train, validate, test = np.split(dataset.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "# train, test = train_test_split(shuffled_dataset, test_size=0.4)\n",
    "# test, validate = train_test_split(test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prostate-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train[\"data\"], train[\"label\"]\n",
    "X_val, y_val = validate[\"data\"], validate[\"label\"]\n",
    "X_test, y_test = test[\"data\"], test[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expired-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "X_val = X_val[:1000]\n",
    "y_val = y_val[:1000]\n",
    "X_test = X_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "independent-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 512    # Bert Max Len input\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=True)\n",
    "\n",
    "pre_train_input_ids = []\n",
    "pre_val_input_ids = []\n",
    "pre_test_input_ids = []\n",
    "\n",
    "for sent in X_train:\n",
    "    tokenized_text = tokenizer.encode(\n",
    "                                    sent,                      # Sentence to encode\n",
    "                                    add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens\n",
    "                                    max_length = MAX_LEN,      # Truncate senences\n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "    pre_train_input_ids.append(tokenized_text)\n",
    "    \n",
    "for sent in X_val:\n",
    "    tokenized_text = tokenizer.encode(\n",
    "                                    sent,                    \n",
    "                                    add_special_tokens = True, \n",
    "                                    max_length = MAX_LEN,\n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "    pre_val_input_ids.append(tokenized_text)\n",
    "\n",
    "for sent in X_test:\n",
    "    tokenized_text = tokenizer.encode(\n",
    "                                    sent,                     \n",
    "                                    add_special_tokens = True, \n",
    "                                    max_length = MAX_LEN,        \n",
    "                                    truncation=True,\n",
    "                                    )\n",
    "    pre_test_input_ids.append(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "metropolitan-pension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pre_train_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "persistent-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_n_pad(input_id_list):\n",
    "    ret_list = []\n",
    "    for input_id in input_id_list:\n",
    "        if len(input_id) > MAX_LEN:\n",
    "            ret_list.append(input_id[:MAX_LEN])\n",
    "        elif len(input_id) < MAX_LEN:\n",
    "            temp_sublist = input_id + [0] * (MAX_LEN - len(input_id))\n",
    "            ret_list.append(temp_sublist)\n",
    "        else:\n",
    "            ret_list.append(input_id)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lined-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appears that CS Serv don[t have tf version 2.2]\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Pad our input tokens\n",
    "# train_input_ids = pad_sequences(train_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# val_input_ids = pad_sequences(val_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "train_input_ids = trunc_n_pad(pre_train_input_ids)\n",
    "val_input_ids = trunc_n_pad(pre_val_input_ids)\n",
    "test_input_ids = trunc_n_pad(pre_test_input_ids)\n",
    "\n",
    "# Create attention masks\n",
    "train_attention_masks = []\n",
    "val_attention_masks = []\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in train_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    train_attention_masks.append(seq_mask)\n",
    "for seq in val_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    val_attention_masks.append(seq_mask)\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "opening-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(train_input_ids[0])\n",
    "print(len(train_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "monetary-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_input_ids)\n",
    "validation_inputs = torch.tensor(val_input_ids)\n",
    "\n",
    "train_labels = torch.tensor(y_train.values.tolist())\n",
    "validation_labels = torch.tensor(y_val.values.tolist())\n",
    "\n",
    "train_masks = torch.tensor(train_attention_masks)\n",
    "validation_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(y_test.values.tolist())\n",
    "\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "personalized-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_inputs), len(train_masks), len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lesbian-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator of our data with torch DataLoader. \n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "prediction_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# dataset_dir = 'dataset/{}'.format(args.dataset)\n",
    "# if not os.path.exists(dataset_dir):\n",
    "#     os.makedirs(dataset_dir)\n",
    "\n",
    "# torch.save(train_data, dataset_dir+'/train.pt')\n",
    "# torch.save(validation_data, dataset_dir+'/val.pt')\n",
    "# torch.save(prediction_data, dataset_dir+'/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "electrical-corrections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.sampler.RandomSampler object at 0x7fe5d2577850>\n"
     ]
    }
   ],
   "source": [
    "print(train_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "north-apache",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "frozen-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cleared-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 3\n",
    "weight_decay = 0.2\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=learning_rate, eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "t_total = len(train_dataloader) * epochs\n",
    "# Store our loss and accuracy for plotting\n",
    "\n",
    "best_val = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "measured-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1501, in forward\n    outputs = self.bert(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 971, in forward\n    encoder_outputs = self.encoder(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 568, in forward\n    layer_outputs = layer_module(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 456, in forward\n    self_attention_outputs = self.attention(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 387, in forward\n    self_outputs = self.self(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 309, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 2.08 GiB already allocated; 72.44 MiB free; 2.12 GiB reserved in total by PyTorch)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-699155133be5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 1501, in forward\n    outputs = self.bert(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 971, in forward\n    encoder_outputs = self.encoder(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 568, in forward\n    layer_outputs = layer_module(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 456, in forward\n    self_attention_outputs = self.attention(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 387, in forward\n    self_outputs = self.self(\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"/u/lab/jw6qs/.conda/envs/ml_proj/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\", line 309, in forward\n    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\nRuntimeError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.76 GiB total capacity; 2.08 GiB already allocated; 72.44 MiB free; 2.12 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for epoch in trange(epochs, desc=\"Epoch\"): \n",
    "# Training\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    # Tracking variables\n",
    "    tr_loss =  0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    model.train()\n",
    "\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        loss_ce = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            loss_ce = loss_ce.mean()\n",
    "        loss_ce.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss_ce.item()\n",
    "\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    print(\"Train cross entropy loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    # Validation\n",
    "    # Put model in evaluation mode to evaluate loss on the validation set\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_accurate_nb = 0\n",
    "    nb_eval_examples = 0\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0] \n",
    "            logits_list.append(logits)\n",
    "            labels_list.append(b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_nb = accurate_nb(logits, label_ids)\n",
    "\n",
    "        eval_accurate_nb += tmp_eval_nb\n",
    "        nb_eval_examples += label_ids.shape[0]\n",
    "    eval_accuracy = eval_accurate_nb/nb_eval_examples\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "    scheduler.step(eval_accuracy)\n",
    "\n",
    "    logits_ece = torch.cat(logits_list)\n",
    "    labels_ece = torch.cat(labels_list)\n",
    "    ece = ece_criterion(logits_ece, labels_ece).item()\n",
    "    print('ECE on val data: {}'.format(ece))\n",
    "\n",
    "\n",
    "    if eval_accuracy > best_val:\n",
    "        dirname = '{}/BERT-base-{}'.format(dataset, seed)\n",
    "\n",
    "        output_dir = './model_save/{}'.format(dirname)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(\"Saving model to %s\" % output_dir)\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model \n",
    "        model_to_save.save_pretrained(output_dir)   \n",
    "        #tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "        best_val = eval_accuracy\n",
    "\n",
    "# ##### test model on test data\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_accurate_nb = 0\n",
    "    nb_test_examples = 0\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "            logits_list.append(logits)\n",
    "            labels_list.append(b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_nb = accurate_nb(logits, label_ids)\n",
    "        eval_accurate_nb += tmp_eval_nb\n",
    "        nb_test_examples += label_ids.shape[0]\n",
    "\n",
    "    print(\"Test Accuracy: {}\".format(eval_accurate_nb/nb_test_examples))\n",
    "\n",
    "    logits_ece = torch.cat(logits_list)\n",
    "    labels_ece = torch.cat(labels_list)\n",
    "    ece = ece_criterion(logits_ece, labels_ece).item()\n",
    "\n",
    "    print('ECE on test data: {}'.format(ece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old Code below... delete soonish???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the accuracy of our predictions vs labels\n",
    "# def accuracy(preds, labels):\n",
    "#     pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = [] \n",
    "\n",
    "# # Store our loss and accuracy for plotting\n",
    "# train_loss_set = []\n",
    "\n",
    "# # Number of training epochs (authors recommend between 2 and 4)\n",
    "# epochs = 4\n",
    "\n",
    "# # trange is a tqdm wrapper around the normal python range\n",
    "# for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "#   # Training\n",
    "  \n",
    "#   # Set our model to training mode (as opposed to evaluation mode)\n",
    "#   model.train()\n",
    "  \n",
    "#   # Tracking variables\n",
    "#   tr_loss = 0\n",
    "#   nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "#   # Train the data for one epoch\n",
    "#   for step, batch in enumerate(train_dataloader):\n",
    "#     # Add batch to GPU\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     # Unpack the inputs from our dataloader\n",
    "#     b_input_ids, b_input_mask, b_labels = batch\n",
    "#     # Clear out the gradients (by default they accumulate)\n",
    "#     optimizer.zero_grad()\n",
    "#     # Forward pass\n",
    "#     loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "#     train_loss_set.append(loss.item())    \n",
    "#     # Backward pass\n",
    "#     loss.backward()\n",
    "#     # Update parameters and take a step using the computed gradient\n",
    "#     optimizer.step()\n",
    "    \n",
    "    \n",
    "#     # Update tracking variables\n",
    "#     tr_loss += loss.item()\n",
    "#     nb_tr_examples += b_input_ids.size(0)\n",
    "#     nb_tr_steps += 1\n",
    "\n",
    "#   print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "#   # Validation\n",
    "\n",
    "#   # Put model in evaluation mode to evaluate loss on the validation set\n",
    "#   model.eval()\n",
    "\n",
    "#   # Tracking variables \n",
    "#   eval_loss, eval_accuracy = 0, 0\n",
    "#   nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#   # Evaluate data for one epoch\n",
    "#   for batch in validation_dataloader:\n",
    "#     # Add batch to GPU\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "#     # Unpack the inputs from our dataloader\n",
    "#     b_input_ids, b_input_mask, b_labels = batch\n",
    "#     # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "#     with torch.no_grad():\n",
    "#       # Forward pass, calculate logit predictions\n",
    "#       logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "#     # Move logits and labels to CPU\n",
    "#     logits = logits.detach().cpu().numpy()\n",
    "#     label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "#     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "#     eval_accuracy += tmp_eval_accuracy\n",
    "#     nb_eval_steps += 1\n",
    "\n",
    "#   print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Batch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.plot(train_loss_set)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
