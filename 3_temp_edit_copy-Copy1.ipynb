{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65992955",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.10.2-cp38-cp38-manylinux2010_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.10.2\n",
      "    Uninstalling tokenizers-0.10.2:\n",
      "      Successfully uninstalled tokenizers-0.10.2\n",
      "Successfully installed tokenizers-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3eb048a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: aiohttp in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (3.7.4)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (1.0.2)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.10.1)\n",
      "Requirement already satisfied: filelock in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (5.3.1)\n",
      "Requirement already satisfied: gpustat in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.6.0)\n",
      "Requirement already satisfied: colorama in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.28.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (1.37.1)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.3.5)\n",
      "Requirement already satisfied: aiohttp-cors in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.7.0)\n",
      "Requirement already satisfied: aioredis in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (1.3.1)\n",
      "Requirement already satisfied: requests in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (2.25.1)\n",
      "Requirement already satisfied: jsonschema in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (1.20.2)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (3.15.8)\n",
      "Requirement already satisfied: opencensus in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (0.7.12)\n",
      "Requirement already satisfied: redis>=3.5.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (3.5.3)\n",
      "Requirement already satisfied: click>=7.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from ray) (7.1.2)\n",
      "Requirement already satisfied: six>=1.5.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (3.7.4.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (20.3.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (4.0.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->ray) (1.6.3)\n",
      "Requirement already satisfied: idna>=2.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp->ray) (2.10)\n",
      "Requirement already satisfied: hiredis in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aioredis->ray) (2.0.0)\n",
      "Requirement already satisfied: blessings>=1.6 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from gpustat->ray) (1.7)\n",
      "Requirement already satisfied: psutil in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from gpustat->ray) (5.8.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from gpustat->ray) (7.352.0)\n",
      "Requirement already satisfied: setuptools in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from jsonschema->ray) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from jsonschema->ray) (0.17.3)\n",
      "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from opencensus->ray) (1.26.3)\n",
      "Requirement already satisfied: opencensus-context==0.1.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from opencensus->ray) (0.1.2)\n",
      "Requirement already satisfied: pytz in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2021.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.30.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (20.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from packaging>=14.3->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.21.1->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->ray) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->ray) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6be225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (0.8.9)\n",
      "Requirement already satisfied: torch in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing_extensions in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from torch) (1.20.2)\n",
      "Requirement already satisfied: pytorch_lightning in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (1.2.10)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (1.20.2)\n",
      "Requirement already satisfied: packaging in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (20.9)\n",
      "Requirement already satisfied: future>=0.17.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: torch>=1.4 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (2021.4.0)\n",
      "Requirement already satisfied: torchmetrics==0.2.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (4.60.0)\n",
      "Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (2.4.1)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pytorch_lightning) (5.3.1)\n",
      "Requirement already satisfied: aiohttp in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.4)\n",
      "Requirement already satisfied: requests in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.30.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (49.6.0.post20210108)\n",
      "Requirement already satisfied: absl-py>=0.4 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.3.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.37.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.15.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.36.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.15.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: typing_extensions in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from torch>=1.4->pytorch_lightning) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from packaging->pytorch_lightning) (2.4.7)\n",
      "Requirement already satisfied: pandas in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: transformers in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/transformers-4.4.2-py3.8.egg (4.4.2)\n",
      "Requirement already satisfied: filelock in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (1.20.2)\n",
      "Requirement already satisfied: packaging in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/sacremoses-0.0.43-py3.8.egg (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: six in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: numpy in /localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages (1.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate\n",
    "!pip install torch\n",
    "!pip install pytorch_lightning\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5cf5fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "#!pip install pytorch-lightning transformers keras matplotlib numpy sklearn nltk pytorch-pretrained-bert pytorch-nlp matplotlib sklearn keras\n",
    "#Lightning Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "import ray.tune\n",
    "from functools import partial\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "#Other Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification\n",
    "#from transformers import BerttokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pickle\n",
    "\n",
    "#from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "\n",
    "# from argparse import ArgumentParser\n",
    "\n",
    "config = {\n",
    "    #\"lr\": ray.tune.loguniform(float = .005, float = 5e-5),\n",
    "    'lr' : 2e-5,\n",
    "    \"weight_decay\": 0.2,\n",
    "    \"adam_epsilon\" : 1e-9,\n",
    "    \"warmup_steps\" : 0,\n",
    "    #\"BATCH_SIZE\": tune.choice([32,64,128]),\n",
    "    #\"MAX_EPOCHS\": ray.tune.choice([2, 5, 8])\n",
    "}\n",
    "\n",
    "class ProjData(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str = \"~/tmp\", batch_size: int = 32, max_len : int = 128, ratio : int = 2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        # self.batch_size = batch_size\n",
    "        self.max_len = max_len # Bert Max Len input\n",
    "        self.rat = ratio\n",
    "        self.name = \"a-\" + str(max_len) + \"-\" + str(ratio)\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        # *** tokenizer isn't actually a constant and the do_lower_case should be redundant if preprocessing was correct.\n",
    "        # self.tokenizer = BerttokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True, do_lower_case=True)\n",
    "\n",
    "\n",
    "        ###\n",
    "        # Preprocessing Data\n",
    "        ###\n",
    "        print(\"Getting pos\")\n",
    "        pos_data = self.get_pos_dataset()\n",
    "        print(\"Getting neg\")\n",
    "        neg_data = self.get_neg_dataset()\n",
    "\n",
    "        # *** TODO: Proper subset selection either in concat_datasets or in get_neg_dataset\n",
    "        print(\"Joining\")\n",
    "        dataset = self.concat_datasets(pos_data, neg_data)\n",
    "        dataset.dropna(inplace=True) # Losing negative examples potentially...\n",
    "        dataset[\"data\"] = dataset[\"data\"].map(self.preprocess)\n",
    "\n",
    "        # 60% - train set, 20% - validation set, 20% - test set\n",
    "        train, validate, test = np.split(dataset.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "\n",
    "        X_train, y_train = train[\"data\"], train[\"label\"]\n",
    "        X_val, y_val = validate[\"data\"], validate[\"label\"]\n",
    "        X_test, y_test = test[\"data\"], test[\"label\"]    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # NOTE: This is a small subset used for testing... likely will remove in final ver.\n",
    "        # *** Set to \"None\" to skip - if there any reason you take the tails of the lists rather than the heads?\n",
    "        PROTOTYPE_NUM = None\n",
    "        # **** NEED TO SHUFFLE FIRST IF YOU USE THIS\n",
    "        if PROTOTYPE_NUM:\n",
    "            X_train = X_train[:PROTOTYPE_NUM * 10]\n",
    "            y_train = y_train[:PROTOTYPE_NUM * 10]\n",
    "            X_val = X_val[:PROTOTYPE_NUM]\n",
    "            y_val = y_val[:PROTOTYPE_NUM]\n",
    "            X_test = X_test[:PROTOTYPE_NUM]\n",
    "            y_test = y_test[:PROTOTYPE_NUM]\n",
    "\n",
    "        ###\n",
    "        # Tokenization\n",
    "        ###\n",
    "        # Convert texts into tokens. (These are not truncated or padded yet)\n",
    "        print(\"Tokenizing\")\n",
    "        pre_train_input_ids = self.tokenize_datasets(X_train, self.tokenizer)\n",
    "        pre_val_input_ids = self.tokenize_datasets(X_val, self.tokenizer)\n",
    "        pre_test_input_ids = self.tokenize_datasets(X_test, self.tokenizer)\n",
    "        \n",
    "        # Truncate and Pad your tokens\n",
    "        print(\"Padding\")\n",
    "        train_input_ids = self.trunc_n_pad(pre_train_input_ids)\n",
    "        val_input_ids = self.trunc_n_pad(pre_val_input_ids)\n",
    "        test_input_ids = self.trunc_n_pad(pre_test_input_ids)\n",
    "\n",
    "        ###\n",
    "        # Misc.\n",
    "        ###\n",
    "        # Create attention masks\n",
    "        print(\"Creating masks\")\n",
    "        train_attention_masks = self.create_attention_masks(train_input_ids)\n",
    "        val_attention_masks = self.create_attention_masks(val_input_ids)\n",
    "        test_attention_masks = self.create_attention_masks(test_input_ids)\n",
    "\n",
    "        # # Convert all of our data into torch tensors, the required datatype for our model\n",
    "        train_inputs = torch.tensor(train_input_ids)\n",
    "        validation_inputs = torch.tensor(val_input_ids)\n",
    "\n",
    "        train_labels = torch.tensor(y_train.values.tolist())\n",
    "        validation_labels = torch.tensor(y_val.values.tolist())\n",
    "\n",
    "        train_masks = torch.tensor(train_attention_masks)\n",
    "        validation_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "        test_inputs = torch.tensor(test_input_ids)\n",
    "        test_labels = torch.tensor(y_test.values.tolist())\n",
    "\n",
    "        test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "        # Create an iterator of our data with torch DataLoader. \n",
    "        self.train = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        self.val = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        self.test = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "        print(train_inputs)\n",
    "        print(self.train)\n",
    "\n",
    "    def process(self, sent):\n",
    "        s = self.preprocess(sent)\n",
    "        s = self.tokenize_datasets([s], self.tokenizer)\n",
    "        s = self.trunc_n_pad(s)\n",
    "        mask = self.create_attention_masks(s)\n",
    "        return  torch.tensor(s),  torch.tensor(mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, x):\n",
    "\n",
    "        # self.re1 = re.compile('\\n') \n",
    "        # self.re2 = re.compile('[\\w]+')\n",
    "        x = re.sub(r'(\\n+)',r' ', x)\n",
    "        # lower case in tokenizer\n",
    "        x = \" \".join(re.findall('[\\w]+',x))\n",
    "        return x\n",
    "        \n",
    "        # # RNV uses a special preprocess step --- mirror it for pushio.\n",
    "        # # print(\"Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\")\n",
    "        # ### 1) remove newlines\n",
    "        # data['data'] = data['data'].replace('\\n', ' ', regex = True)\n",
    "\n",
    "        # ## 2) convert to lowercase\n",
    "        # data['data'] = data['data'].str.lower()\n",
    "\n",
    "        # # ### 3) remove punct and numbers: https://stackoverflow.com/questions/47947438/preprocessing-string-data-in-pandas-dataframe\n",
    "        # data[\"data\"] = data.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    " \n",
    "\n",
    "    # Import PushIO CSV    \n",
    "    def get_neg_dataset(self, path=\"/bigtemp/rm5tx/nlp_project/2016-05_all.csv\"):\n",
    "        # path = \"/localtmp/rm5tx/nlp_project/2016-05_all.csv\"\n",
    "        print(\"reading neg\")\n",
    "        data = pd.read_csv(path, usecols=['body'], dtype=\"string\")\n",
    "        data.rename(columns={\"body\":\"data\"}, inplace=True)\n",
    "        # print(\"filtering n/a neg\")\n",
    "        # data.dropna(inplace=True)\n",
    "        # print(\"preprocessing neg\") --- less work to do this on the joined\n",
    "        # data[\"data\"] = data[\"data\"].map(self.preprocess)\n",
    "        data[\"label\"] = 0\n",
    "        # We want a unify col name for when we concat pos and neg data\n",
    "\n",
    "        return data\n",
    "\n",
    "    # Reddit Norm Violations\n",
    "    def get_pos_dataset(self, path=\"/bigtemp/rm5tx/nlp_project/reddit-norm-violations/data/macro-norm-violations/\"):\n",
    "        directory = os.path.abspath(path)\n",
    "\n",
    "        pos_temp = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                with open(root+ \"/\" +file) as f:\n",
    "                    pos_temp += f.readlines()\n",
    "        data = pd.DataFrame(data=pos_temp, dtype = \"string\")\n",
    "        data.rename(columns={0:\"data\"}, inplace=True)\n",
    "        data[\"label\"] = 1\n",
    "        return data\n",
    "\n",
    "    def get_adjacents_datasets(self):\n",
    "        neg_init = self.get_neg_dataset()\n",
    "        pos_init = self.get_neg_dataset()\n",
    "        pos, neg = 0, 0\n",
    "        return pos, neg\n",
    "\n",
    "    def concat_datasets(self, data_a, data_b):\n",
    "        if len(data_a.index) > len(data_b.index):\n",
    "            data_a, data_b = data_b, data_a\n",
    "        # frames = [data_a, data_b[(self.rat * len(data_a.index)):]]\n",
    "        trunced_b = data_b.sample(n=int(self.rat * len(data_a.index)), random_state=42)\n",
    "        print(\"Using \", len(data_a.index), len(trunced_b.index), \" samples.\")\n",
    "        dataset = pd.concat([data_a, trunced_b])\n",
    "        print(\"Total = \", len(dataset.index))\n",
    "        return dataset\n",
    "\n",
    "    ###Pre-processing Code###\n",
    "    def tokenize_datasets(self, X_dataset, tokenizer):\n",
    "        input_ids = []\n",
    "        for sent in X_dataset:\n",
    "            tokenized_text = tokenizer.encode(\n",
    "                                            sent,                      # Sentence to encode\n",
    "                                            add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens\n",
    "                                            max_length = self.max_len,      # Truncate senences\n",
    "                                            truncation=True,\n",
    "                                            )\n",
    "            input_ids.append(tokenized_text)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "    # Appears that CS Serv don[t have tf version 2.2]\n",
    "    # Thus, we cannot use the convenient pad_sequences from keras\n",
    "    def trunc_n_pad(self, input_id_list):\n",
    "        ret_list = []\n",
    "        for input_id in input_id_list:\n",
    "            if len(input_id) > self.max_len:\n",
    "                ret_list.append(input_id[:self.max_len])\n",
    "            elif len(input_id) < self.max_len:\n",
    "                temp_sublist = input_id + [0] * (self.max_len - len(input_id))\n",
    "                ret_list.append(temp_sublist)\n",
    "            else:\n",
    "                ret_list.append(input_id)\n",
    "        return ret_list\n",
    "\n",
    "    # Create attention masks\n",
    "    def create_attention_masks(self, input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            # Create a mask of 1s for each token followed by 0s for padding\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def train_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.val, batch_size=batch_size)\n",
    "\n",
    "    def test_dataloader(self, batch_size=32):\n",
    "        return DataLoader(self.test, batch_size=sbatch_size)\n",
    "\n",
    "    def save(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        path = \"/bigtemp/jw6qs/\"\n",
    "\n",
    "        torch.save(self.tokenizer, open(path+self.name+\"token.p\", \"wb\"))\n",
    "        torch.save(self.train, open(path+self.name+\"train.pt\", \"wb\"))\n",
    "        torch.save(self.val, open(path+self.name+\"val.pt\", \"wb\"))\n",
    "        torch.save(self.test, open(path+self.name+\"test.pt\", \"wb\"))\n",
    "\n",
    "    def load(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        path = \"/bigtemp/jw6qs/\"\n",
    "\n",
    "        self.tokenizer = torch.load(open(path+self.name+\"token.p\", \"rb\"))\n",
    "        self.train = torch.load(open(path+self.name+\"train.pt\", \"rb\"))\n",
    "        self.val = torch.load(open(path+self.name+\"val.pt\", \"rb\"))\n",
    "        self.test = torch.load(open(path+self.name+\"test.pt\", \"rb\"))    \n",
    "\n",
    "class ProjModel(pl.LightningModule):\n",
    "    def __init__(self,config, weight_decay = config['weight_decay'], learning_rate = config['lr'], \n",
    "                 adam_epsilon = config['adam_epsilon'], warmup_steps = config['warmup_steps']):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_labels = int = 2\n",
    "        #self.learning_rate = config['lr']\n",
    "        #self.adam_epsilon = float = 1e-9\n",
    "        #self.warmup_steps = int = 0\n",
    "        #self.weight_decay = float = 0.2\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                                   num_labels=self.num_labels)\n",
    "        # self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        # self.metric = ...\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        logits = self.model(x, attention_mask=mask).logits\n",
    "        # return logits        \n",
    "        pred = torch.argmax(logits, 1)\n",
    "        return pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        out = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        logits = out.logits\n",
    "        loss = out.loss\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('train_acc', acc)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # *** Old Monitor, NYI -- Do we want to use a scheduler at all?\n",
    "    # https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\n",
    "    #     # for batch in validation_dataloader:\n",
    "    #     #     with torch.no_grad():\n",
    "    #     #         logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0] \n",
    "\n",
    "    #     #     tmp_eval_nb = accurate_nb(logits, b_labels)\n",
    "\n",
    "    #     #     eval_accurate_nb += tmp_eval_nb\n",
    "    #     #     nb_eval_examples += label_ids.shape[0]\n",
    "    #     # eval_accuracy = eval_accurate_nb/nb_eval_examples\n",
    "    #     # print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "    #     # scheduler.step(eval_accuracy)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask).logits\n",
    "        \n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('valid_acc', acc)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask).logits\n",
    "\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('test_acc', acc)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ###\n",
    "        # Param Optim.\n",
    "        ###\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': self.hparams.weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "        # return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'eval_acc'}\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def accurate_nb(self, logits, labels):\n",
    "        pred_flat = torch.argmax(logits, dim=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return torch.sum(pred_flat == labels_flat) / labels_flat.shape[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25c6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune(config, data):\n",
    "    model = ProjModel(config)\n",
    "    metrics = {\"loss\": \"avg_val_loss\", \"acc\": \"mean_accuracy\"}\n",
    "#     callback = [TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "    callback = [TuneReportCallback({\n",
    "    \"loss\": \"avg_val_loss\",\n",
    "    \"mean_accuracy\": \"avg_val_accuracy\"\n",
    "    }, on=\"validation_end\")]\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"nlp_proj\")\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 32\n",
    "    TEST_BATCH_SIZE = 32\n",
    "    \n",
    "    trainer = pl.Trainer(logger=logger, \n",
    "                            accelerator='dp', # jupyter can't use ddp, use dp instead\n",
    "                            # effective batch size is batch_size * num_gpus * num_nodes\n",
    "                            gpus=1, \n",
    "                            gradient_clip_val=1.0, \n",
    "                            fast_dev_run=False,\n",
    "                            progress_bar_refresh_rate=0,\n",
    "                            callbacks= callback\n",
    "                        )\n",
    "    trainer.fit(model, data.train_dataloader(batch_size=TRAIN_BATCH_SIZE), \n",
    "                    data.val_dataloader(batch_size=VAL_BATCH_SIZE))\n",
    "    \n",
    "\n",
    "    \n",
    "    reporter = CLIReporter(metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
    "    \n",
    "    analysis = ray.tune.run(\n",
    "        ray.tune.with_parameters(train_tune, epochs = 10),\n",
    "        resources_per_trial={\"cpu\": 2, \"gpu\": 2},\n",
    "        config = config,\n",
    "        num_samples = 5,\n",
    "        progress_reporter = reporter\n",
    "    )\n",
    "    print(analysis.best_config)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adac2798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 32\n",
    "    TEST_BATCH_SIZE = 32\n",
    "\n",
    "    #LEARNING_RATE = 5e-5\n",
    "    #MAX_EPOCHS = 2\n",
    "    #WEIGHT_DECAY = 0.2\n",
    "        \n",
    "    SEED = 7\n",
    "    # DATA_PATH = \"/bigtemp/rm5tx/nlp_project/data_cache/\"\n",
    "    DATA_PATH = \"/localtmp/rm5tx/nlp_project/data_cache/\"\n",
    "    \n",
    "    data = ProjData(max_len=128, ratio=2)\n",
    "    try:\n",
    "        data.load(DATA_PATH)\n",
    "        print(\"Loaded Saved Data\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        data.setup()\n",
    "        data.save(DATA_PATH)\n",
    "        \n",
    "    train_tune(config, data)\n",
    "    ### Comment out the try block and uncomment below while you're working on the data part or you'll just skip it and use old data.\n",
    "    # data.setup() \n",
    "    # data.save(DATA_PATH)\n",
    "    \n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    sents = [\"random sentence\", \"pretty flowers\", \"idiot\", \"fuck you cunt nigger\"]\n",
    "    for sent in sents:\n",
    "        x, mask = data.process(sent)\n",
    "        print(model(x, mask))\n",
    "                      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7520c81d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Saved Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 109 M \n",
      "--------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'avg_val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5b52d35e4541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1c9143a4958c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m### Comment out the try block and uncomment below while you're working on the data part or you'll just skip it and use old data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# data.setup()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0dd7492d6b75>\u001b[0m in \u001b[0;36mtrain_tune\u001b[0;34m(config, data)\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                         )\n\u001b[0;32m---> 24\u001b[0;31m     trainer.fit(model, data.train_dataloader(batch_size=TRAIN_BATCH_SIZE), \n\u001b[0m\u001b[1;32m     25\u001b[0m                     data.val_dataloader(batch_size=VAL_BATCH_SIZE))\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_or_test_or_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_check_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# reset stage to train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, max_batches, on_epoch)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_evaluation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;31m# log epoch metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mon_evaluation_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_test_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_validation_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreload_evaluation_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mtrainer_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m                 \u001b[0mtrainer_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;34m\"\"\"Called when the validation loop ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/ray/tune/integration/pytorch_lightning.py\u001b[0m in \u001b[0;36mon_validation_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"validation_end\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/localtmp/jw6qs/conda/envs/slocal/lib/python3.8/site-packages/ray/tune/integration/pytorch_lightning.py\u001b[0m in \u001b[0;36m_handle\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mreport_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mreport_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'avg_val_loss'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17898f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
