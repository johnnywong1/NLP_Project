{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: matplotlib in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: transformers in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: torch in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: sklearn in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Requirement already satisfied: nltk in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages\n",
      "Collecting pytorch-pretrained-bert\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-nlp\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/51/f0ee1efb75f7cc2e3065c5da1363d6be2eec79691b2821594f3f2329528c/pytorch_nlp-0.5.0-py3-none-any.whl (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: h5py in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: scipy>=0.14 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: python-dateutil>=2.0 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pytz in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: cycler>=0.10 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from matplotlib)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: tqdm>=4.27 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: packaging in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: filelock in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: requests in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: sacremoses in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: scikit-learn in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from sklearn)\n",
      "Collecting boto3 (from pytorch-pretrained-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/98/e9459d65ad8ab27886bf9d86a537e6f65b5bdbc4c7de68ba45b524ef74a1/boto3-1.17.52-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "Requirement already satisfied: zipp>=0.5 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: joblib in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from sacremoses->transformers)\n",
      "Requirement already satisfied: click in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from sacremoses->transformers)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /u/zm8bh/.conda/envs/ml_proj/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->pytorch-pretrained-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/00/89/0cb4e92c239e6425b9b0035227b8cdf9d3d098a5c9e95632c3815df63a09/s3transfer-0.3.7-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.21.0,>=1.20.52 (from boto3->pytorch-pretrained-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/f3/be57d6b74618db08bcc67be52da014c8d12132fedb1d12457c386b7be373/botocore-1.20.52-py2.py3-none-any.whl (7.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.4MB 178kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
      "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\n",
      "Successfully installed boto3-1.17.52 botocore-1.20.52 jmespath-0.10.0 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.7\n",
      "\u001b[33mYou are using pip version 20.0.2, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras matplotlib transformers numpy torch sklearn nltk pytorch-pretrained-bert pytorch-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ac553e17572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloggers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorBoardLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "#If there's a GPU available...\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():        \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")    \n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())    \n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 506 ms, sys: 97.1 ms, total: 603 ms\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import PushIO CSV\n",
    "import pandas as pd\n",
    "\n",
    "def get_pushio_dataset(path=\"\"):\n",
    "    if path:\n",
    "        neg_data = pd.read_csv(path, usecols=['body'], dtype=\"string\")\n",
    "    else:\n",
    "        neg_data = pd.read_csv(\"/bigtemp/rm5tx/nlp_project/2016-05_all.csv\", usecols=['body'], dtype=\"string\")\n",
    "    \n",
    "    # We want a unify col name for when we concat pos and neg data\n",
    "    neg_data.rename(columns={\"body\":\"data\"}, inplace=True)\n",
    "    neg_data[\"label\"] = 0\n",
    "    return neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 7.63 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reddit Norm Violations\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_rnv_dataset(path=\"\"):\n",
    "    if path:\n",
    "        directory = os.path.abspath(path)\n",
    "    else:\n",
    "        directory = os.path.abspath(\"/bigtemp/rm5tx/nlp_project/reddit-norm-violations/data/macro-norm-violations/\")\n",
    "\n",
    "    pos_temp = []\n",
    "    for root,dirs,files in os.walk(directory):\n",
    "        for file in files:\n",
    "            with open(root+ \"/\" +file) as f:\n",
    "                pos_temp += f.readlines()\n",
    "    pos_data = pd.DataFrame(data=pos_temp, dtype = \"string\")\n",
    "    pos_data.rename(columns={0:\"data\"}, inplace=True)\n",
    "    pos_data[\"label\"] = 1\n",
    "    \n",
    "    # RNV uses a special preprocess step\n",
    "    print(\"Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\")\n",
    "    ### 1) remove newlines\n",
    "    pos_data['data'] = pos_data['data'].replace('\\n', ' ', regex = True)\n",
    "\n",
    "    ## 2) convert to lowercase\n",
    "    pos_data['data'] = pos_data['data'].str.lower()\n",
    "\n",
    "    # ### 3) remove punct and numbers: https://stackoverflow.com/questions/47947438/preprocessing-string-data-in-pandas-dataframe\n",
    "    pos_data[\"data\"] = pos_data.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "    return pos_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_datasets(data_a, data_b):\n",
    "    frames = [data_a, data_b]\n",
    "    dataset = pd.concat(frames)\n",
    "    dataset.dropna(inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 6.78 ms, total: 184 ms\n",
      "Wall time: 565 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_LEN = 32  # Bert Max Len input\n",
    "TOKENIZER = BertTokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=True)\n",
    "\n",
    "def tokenize_datasets(X_dataset, tokenizer, max_len=512):\n",
    "    input_ids = []\n",
    "    for sent in X_dataset:\n",
    "        tokenized_text = tokenizer.encode(\n",
    "                                        sent,                      # Sentence to encode\n",
    "                                        add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens\n",
    "                                        max_length = max_len,      # Truncate senences\n",
    "                                        truncation=True,\n",
    "                                        )\n",
    "        input_ids.append(tokenized_text)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 17.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Appears that CS Serv don[t have tf version 2.2]\n",
    "# Thus, we cannot use the convenient pad_sequences from keras\n",
    "\n",
    "def trunc_n_pad(input_id_list):\n",
    "    ret_list = []\n",
    "    for input_id in input_id_list:\n",
    "        if len(input_id) > MAX_LEN:\n",
    "            ret_list.append(input_id[:MAX_LEN])\n",
    "        elif len(input_id) < MAX_LEN:\n",
    "            temp_sublist = input_id + [0] * (MAX_LEN - len(input_id))\n",
    "            ret_list.append(temp_sublist)\n",
    "        else:\n",
    "            ret_list.append(input_id)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "def create_attention_masks(input_ids):\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        # Create a mask of 1s for each token followed by 0s for padding\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accurate_nb(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VAL_BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 3\n",
    "WEIGHT_DECAY = 0.2\n",
    "\n",
    "SEED = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 4 GPUs!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.73 GiB total capacity; 362.40 MiB already allocated; 5.56 MiB free; 392.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 10.73 GiB total capacity; 362.40 MiB already allocated; 5.56 MiB free; 392.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "from torch import nn\n",
    "from tqdm import trange \n",
    "\n",
    "def main():\n",
    "    \n",
    "    ###\n",
    "    # Preprocessing Data\n",
    "    ###\n",
    "    neg_data = get_pushio_dataset()\n",
    "    pos_data = get_rnv_dataset()\n",
    "    dataset = concat_datasets(neg_data, pos_data)\n",
    "\n",
    "    # 60% - train set, 20% - validation set, 20% - test set\n",
    "    train, validate, test = np.split(dataset.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "\n",
    "    X_train, y_train = train[\"data\"], train[\"label\"]\n",
    "    X_val, y_val = validate[\"data\"], validate[\"label\"]\n",
    "    X_test, y_test = test[\"data\"], test[\"label\"]\n",
    "\n",
    "    # NOTE: This is a small subset used for testing... likely will remove in final ver.\n",
    "    X_train = X_train[:1000]\n",
    "    y_train = y_train[:1000]\n",
    "    X_val = X_val[:1000]\n",
    "    y_val = y_val[:1000]\n",
    "    X_test = X_test[:1000]\n",
    "    y_test = y_test[:1000]\n",
    "\n",
    "    ###\n",
    "    # Tokenization\n",
    "    ###\n",
    "    # Convert texts into tokens. (These are not truncated or padded yet)\n",
    "    pre_train_input_ids = tokenize_datasets(X_train, TOKENIZER, MAX_LEN)\n",
    "    pre_val_input_ids = tokenize_datasets(X_val, TOKENIZER, MAX_LEN)\n",
    "    pre_test_input_ids = tokenize_datasets(X_test, TOKENIZER, MAX_LEN)\n",
    "    \n",
    "    # Truncate and Pad your tokens\n",
    "    train_input_ids = trunc_n_pad(pre_train_input_ids)\n",
    "    val_input_ids = trunc_n_pad(pre_val_input_ids)\n",
    "    test_input_ids = trunc_n_pad(pre_test_input_ids)\n",
    "\n",
    "    ###\n",
    "    # Misc.\n",
    "    ###\n",
    "    # Create attention masks\n",
    "    train_attention_masks = create_attention_masks(train_input_ids)\n",
    "    val_attention_masks = create_attention_masks(val_input_ids)\n",
    "    test_attention_masks = create_attention_masks(test_input_ids)\n",
    "    \n",
    "    # Convert all of our data into torch tensors, the required datatype for our model\n",
    "    train_inputs = torch.tensor(train_input_ids)\n",
    "    validation_inputs = torch.tensor(val_input_ids)\n",
    "\n",
    "    train_labels = torch.tensor(y_train.values.tolist())\n",
    "    validation_labels = torch.tensor(y_val.values.tolist())\n",
    "\n",
    "    train_masks = torch.tensor(train_attention_masks)\n",
    "    validation_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "    test_inputs = torch.tensor(test_input_ids)\n",
    "    test_labels = torch.tensor(y_test.values.tolist())\n",
    "\n",
    "    test_masks = torch.tensor(test_attention_masks)\n",
    "    \n",
    "    # Create an iterator of our data with torch DataLoader. \n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    prediction_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    \n",
    "    # Create Dataloaders- a Python iterable over a dataset\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
    "\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=TEST_BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Model And Param Optim.\n",
    "    ###\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate': WEIGHT_DECAY},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=LEARNING_RATE, eps=1e-9)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "    t_total = len(train_dataloader) * EPOCHS\n",
    "    # Store our loss and accuracy for plotting\n",
    "\n",
    "    best_val = -np.inf\n",
    "    \n",
    "    # trange is a tqdm wrapper around the normal python range\n",
    "    for epoch in trange(EPOCHS, desc=\"Epoch\"): \n",
    "    # Training\n",
    "        # Set our model to training mode (as opposed to evaluation mode)\n",
    "        # Tracking variables\n",
    "        tr_loss =  0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        model.train()\n",
    "\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            loss_ce = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                loss_ce = loss_ce.mean()\n",
    "            loss_ce.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss_ce.item()\n",
    "\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        print(\"Train cross entropy loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "        # Validation\n",
    "        # Put model in evaluation mode to evaluate loss on the validation set\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        eval_accurate_nb = 0\n",
    "        nb_eval_examples = 0\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0] \n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(b_labels)\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_nb = accurate_nb(logits, label_ids)\n",
    "\n",
    "            eval_accurate_nb += tmp_eval_nb\n",
    "            nb_eval_examples += label_ids.shape[0]\n",
    "        eval_accuracy = eval_accurate_nb/nb_eval_examples\n",
    "        print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "        scheduler.step(eval_accuracy)\n",
    "\n",
    "\n",
    "        if eval_accuracy > best_val:\n",
    "            dirname = '{}/BERT-base-{}'.format(dataset, SEED)\n",
    "            # Directory names at longest can be 255\n",
    "            dirname = dirname[:255]\n",
    "            output_dir = './model_save/{}'.format(dirname)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            print(\"Saving model to %s\" % output_dir)\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model \n",
    "            model_to_save.save_pretrained(output_dir)   \n",
    "            #tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            best_val = eval_accuracy\n",
    "\n",
    "    # ##### test model on test data\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        eval_accurate_nb = 0\n",
    "        nb_test_examples = 0\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "        # Predict \n",
    "        for batch in prediction_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions \n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(b_labels)\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_nb = accurate_nb(logits, label_ids)\n",
    "            eval_accurate_nb += tmp_eval_nb\n",
    "            nb_test_examples += label_ids.shape[0]\n",
    "\n",
    "        print(\"Test Accuracy: {}\".format(eval_accurate_nb/nb_test_examples))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
