{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (1.2.7)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (1.8.1+cu101)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (4.60.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (5.3.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (2.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (1.20.2)\n",
      "Requirement already satisfied: aiohttp in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: requests in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.37.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (52.0.0.post20210125)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.15.8)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
      "Requirement already satisfied: pytorch-lightning in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (1.2.7)\n",
      "Requirement already satisfied: transformers in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (4.5.0)\n",
      "Requirement already satisfied: keras in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (2.4.3)\n",
      "Requirement already satisfied: matplotlib in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (1.20.2)\n",
      "Requirement already satisfied: sklearn in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: nltk in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (3.5)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (0.6.2)\n",
      "Requirement already satisfied: pytorch-nlp in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (0.5.0)\n",
      "Requirement already satisfied: h5py in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from keras) (3.2.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from keras) (1.6.2)\n",
      "Requirement already satisfied: pyyaml in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: regex in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from nltk) (4.60.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (1.8.1+cu101)\n",
      "Requirement already satisfied: torchmetrics>=0.2.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.18.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (2.4.1)\n",
      "Requirement already satisfied: fsspec[http]>=0.8.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-lightning) (0.9.0)\n",
      "Requirement already satisfied: requests in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: aiohttp in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.4.post0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.15.8)\n",
      "Requirement already satisfied: wheel>=0.26 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (52.0.0.post20210125)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
      "Requirement already satisfied: boto3 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from pytorch-pretrained-bert) (1.17.46)\n",
      "Requirement already satisfied: scikit-learn in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: filelock in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: packaging in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from aiohttp->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert) (0.3.6)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.46 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert) (1.20.46)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /u/kem5en/.conda/envs/ml_proj2/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "!pip install pytorch-lightning \n",
    "\n",
    "!pip install pytorch-lightning transformers keras matplotlib numpy sklearn nltk pytorch-pretrained-bert pytorch-nlp matplotlib sklearn keras\n",
    "#Lightning Imports\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#Other Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification\n",
    "# from transformers import BerttokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pickle\n",
    "\n",
    "# from torch import nn\n",
    "# from tqdm import trange \n",
    "\n",
    "# Possible Bert-Light imports:\n",
    "# from argparse import ArgumentParser\n",
    "# from datetime import datetime\n",
    "# from typing import Optional\n",
    "\n",
    "# import datasets\n",
    "# import numpy as np\n",
    "# import pytorch_lightning as pl\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import (\n",
    "#     AdamW,\n",
    "#     AutoModelForSequenceClassification,\n",
    "#     AutoConfig,\n",
    "#     AutoTokenizer,\n",
    "#     get_linear_schedule_with_warmup,\n",
    "# )\n",
    "\n",
    "class ProjData(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_dir: str = \"~/tmp\", batch_size: int = 32, max_len : int = 64, ratio : int = 2):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len # Bert Max Len input\n",
    "        self.rat = ratio\n",
    "        self.name = \"a-\" + str(max_len) + \"-\" + str(ratio)\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        \n",
    "        # *** tokenizer isn't actually a constant and the do_lower_case should be redundant if preprocessing was correct.\n",
    "        # self.tokenizer = BerttokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "\n",
    "        ###\n",
    "        # Preprocessing Data\n",
    "        ###\n",
    "        print(\"Getting pos\")\n",
    "        pos_data = self.get_rnv_dataset()\n",
    "        print(\"Getting neg\")\n",
    "        neg_data = self.get_pushio_dataset()\n",
    "\n",
    "        # *** TODO: Proper subset selection either in concat_datasets or in get_pushio_dataset\n",
    "        print(\"Joining\")\n",
    "        dataset = self.concat_datasets(pos_data, neg_data)\n",
    "\n",
    "        # 60% - train set, 20% - validation set, 20% - test set\n",
    "        train, validate, test = np.split(dataset.sample(frac=1, random_state=42), \n",
    "                       [int(.6*len(dataset)), int(.8*len(dataset))])\n",
    "\n",
    "        X_train, y_train = train[\"data\"], train[\"label\"]\n",
    "        X_val, y_val = validate[\"data\"], validate[\"label\"]\n",
    "        X_test, y_test = test[\"data\"], test[\"label\"]    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        # NOTE: This is a small subset used for testing... likely will remove in final ver.\n",
    "        # *** Set to \"None\" to skip - if there any reason you take the tails of the lists rather than the heads?\n",
    "        PROTOTYPE_NUM = None\n",
    "        # **** NEED TO SHUFFLE FIRST IF YOU USE THIS\n",
    "        if PROTOTYPE_NUM:\n",
    "            X_train = X_train[:PROTOTYPE_NUM * 10]\n",
    "            y_train = y_train[:PROTOTYPE_NUM * 10]\n",
    "            X_val = X_val[:PROTOTYPE_NUM]\n",
    "            y_val = y_val[:PROTOTYPE_NUM]\n",
    "            X_test = X_test[:PROTOTYPE_NUM]\n",
    "            y_test = y_test[:PROTOTYPE_NUM]\n",
    "\n",
    "        ###\n",
    "        # Tokenization\n",
    "        ###\n",
    "        # Convert texts into tokens. (These are not truncated or padded yet)\n",
    "        print(\"Tokenizing\")\n",
    "        pre_train_input_ids = self.tokenize_datasets(X_train, self.tokenizer)\n",
    "        pre_val_input_ids = self.tokenize_datasets(X_val, self.tokenizer)\n",
    "        pre_test_input_ids = self.tokenize_datasets(X_test, self.tokenizer)\n",
    "        \n",
    "        # Truncate and Pad your tokens\n",
    "        print(\"Padding\")\n",
    "        train_input_ids = self.trunc_n_pad(pre_train_input_ids)\n",
    "        val_input_ids = self.trunc_n_pad(pre_val_input_ids)\n",
    "        test_input_ids = self.trunc_n_pad(pre_test_input_ids)\n",
    "\n",
    "        ###\n",
    "        # Misc.\n",
    "        ###\n",
    "        # Create attention masks\n",
    "        print(\"Creating masks\")\n",
    "        train_attention_masks = self.create_attention_masks(train_input_ids)\n",
    "        val_attention_masks = self.create_attention_masks(val_input_ids)\n",
    "        test_attention_masks = self.create_attention_masks(test_input_ids)\n",
    "\n",
    "        # # Convert all of our data into torch tensors, the required datatype for our model\n",
    "        train_inputs = torch.tensor(train_input_ids)\n",
    "        validation_inputs = torch.tensor(val_input_ids)\n",
    "\n",
    "        train_labels = torch.tensor(y_train.values.tolist())\n",
    "        validation_labels = torch.tensor(y_val.values.tolist())\n",
    "\n",
    "        train_masks = torch.tensor(train_attention_masks)\n",
    "        validation_masks = torch.tensor(val_attention_masks)\n",
    "\n",
    "        test_inputs = torch.tensor(test_input_ids)\n",
    "        test_labels = torch.tensor(y_test.values.tolist())\n",
    "\n",
    "        test_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "        # Create an iterator of our data with torch DataLoader. \n",
    "        self.train = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        self.val = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "        self.test = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "    def save(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        # token_path = os.path.abspath(path+self.name+\"/tokenizer/\")\n",
    "        # self.tokenizer.save_pretrained(token_path)\n",
    "        torch.save(self.tokenizer, open(path+self.name+\"token.p\", \"wb\"))\n",
    "        torch.save(self.train, open(path+self.name+\"train.pt\", \"wb\"))\n",
    "        torch.save(self.val, open(path+self.name+\"val.pt\", \"wb\"))\n",
    "        torch.save(self.test, open(path+self.name+\"test.pt\", \"wb\"))\n",
    "\n",
    "    def load(self, path=\"/bigtemp/rm5tx/nlp_project/data_cache/\"):\n",
    "        # token_path = os.path.abspath(path+self.name+\"/tokenizer/\")\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(token_path, use_fast=True)\n",
    "        self.tokenizer = torch.load(open(path+self.name+\"token.p\", \"rb\"))\n",
    "        self.train = torch.load(open(path+self.name+\"train.pt\", \"rb\"))\n",
    "        self.val = torch.load(open(path+self.name+\"val.pt\", \"rb\"))\n",
    "        self.test = torch.load(open(path+self.name+\"test.pt\", \"rb\"))     \n",
    "\n",
    "    # Import PushIO CSV    \n",
    "    def get_pushio_dataset(path=None):\n",
    "        # if path:\n",
    "            # neg_data = pd.read_csv(path, usecols=['body'], dtype=\"string\")\n",
    "        # else:\n",
    "        neg_data = pd.read_csv(\"/bigtemp/rm5tx/nlp_project/2016-05_all.csv\", usecols=['body'], dtype=\"string\")\n",
    "    \n",
    "        # We want a unify col name for when we concat pos and neg data\n",
    "        neg_data.rename(columns={\"body\":\"data\"}, inplace=True)\n",
    "        neg_data[\"label\"] = 0\n",
    "        return neg_data\n",
    "\n",
    "    # Reddit Norm Violations\n",
    "    def get_rnv_dataset(path=None):\n",
    "        # if path:\n",
    "        #     directory = os.path.abspath(path)\n",
    "        # else:\n",
    "        directory = os.path.abspath(\"/bigtemp/rm5tx/nlp_project/reddit-norm-violations/data/macro-norm-violations/\")\n",
    "\n",
    "        pos_temp = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                with open(root+ \"/\" +file) as f:\n",
    "                    pos_temp += f.readlines()\n",
    "        pos_data = pd.DataFrame(data=pos_temp, dtype = \"string\")\n",
    "        pos_data.rename(columns={0:\"data\"}, inplace=True)\n",
    "        pos_data[\"label\"] = 1\n",
    "        \n",
    "        # RNV uses a special preprocess step\n",
    "        print(\"Preprocessing... 1. split new lines, 2. convert to lowercase, and 3. strip numbers and punct\")\n",
    "        ### 1) remove newlines\n",
    "        pos_data['data'] = pos_data['data'].replace('\\n', ' ', regex = True)\n",
    "\n",
    "        ## 2) convert to lowercase\n",
    "        pos_data['data'] = pos_data['data'].str.lower()\n",
    "\n",
    "        # ### 3) remove punct and numbers: https://stackoverflow.com/questions/47947438/preprocessing-string-data-in-pandas-dataframe\n",
    "        pos_data[\"data\"] = pos_data.data.apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
    "        return pos_data\n",
    "\n",
    "    def concat_datasets(self, data_a, data_b):\n",
    "        if len(data_a.index) < len(data_b.index):\n",
    "            data_a, data_b = data_b, data_a\n",
    "        frames = [data_a, data_b[(self.rat * len(data_a.index)):]]\n",
    "        print(\"Using \", len(data_a.index), (self.rat * len(data_a.index)), \" samples.\")\n",
    "        dataset = pd.concat(frames)\n",
    "        dataset.dropna(inplace=True) #???\n",
    "        return dataset\n",
    "\n",
    "    ###Pre-processing Code###\n",
    "    def tokenize_datasets(self, X_dataset, tokenizer):\n",
    "        input_ids = []\n",
    "        for sent in X_dataset:\n",
    "            tokenized_text = tokenizer.encode(\n",
    "                                            sent,                      # Sentence to encode\n",
    "                                            add_special_tokens = True, # Add '[CLS]' and '[SEP]' tokens\n",
    "                                            max_length = self.max_len,      # Truncate senences\n",
    "                                            truncation=True,\n",
    "                                            )\n",
    "            input_ids.append(tokenized_text)\n",
    "        return input_ids\n",
    "\n",
    "\n",
    "    # Appears that CS Serv don[t have tf version 2.2]\n",
    "    # Thus, we cannot use the convenient pad_sequences from keras\n",
    "    def trunc_n_pad(self, input_id_list):\n",
    "        ret_list = []\n",
    "        for input_id in input_id_list:\n",
    "            if len(input_id) > self.max_len:\n",
    "                ret_list.append(input_id[:self.max_len])\n",
    "            elif len(input_id) < self.max_len:\n",
    "                temp_sublist = input_id + [0] * (self.max_len - len(input_id))\n",
    "                ret_list.append(temp_sublist)\n",
    "            else:\n",
    "                ret_list.append(input_id)\n",
    "        return ret_list\n",
    "\n",
    "    # Create attention masks\n",
    "    def create_attention_masks(self, input_ids):\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            # Create a mask of 1s for each token followed by 0s for padding\n",
    "            seq_mask = [float(i>0) for i in seq]\n",
    "            attention_masks.append(seq_mask)\n",
    "        return attention_masks\n",
    "\n",
    "    def train_dataloader(self, batch_size):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self, batch_size):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self, batch_size):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)\n",
    "\n",
    "class ProjModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # model_name_or_path: str,\n",
    "        num_labels: int = 2,\n",
    "        learning_rate: float = 2e-5,\n",
    "        adam_epsilon: float = 1e-9,\n",
    "        warmup_steps: int = 0,\n",
    "        weight_decay: float = 0.0,\n",
    "        # eval_splits: Optional[list] = None,\n",
    "        **kwargs\n",
    "        \n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "        # self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n",
    "        # self.metric = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.model(x)\n",
    "        return pred        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('train_acc', acc)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    # *** Old Monitor, NYI -- Do we want to use a scheduler at all?\n",
    "    # https://pytorch-lightning.readthedocs.io/en/latest/common/optimizers.html#automatic-optimization\n",
    "    #     # for batch in validation_dataloader:\n",
    "    #     #     with torch.no_grad():\n",
    "    #     #         logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0] \n",
    "\n",
    "    #     #     tmp_eval_nb = accurate_nb(logits, b_labels)\n",
    "\n",
    "    #     #     eval_accurate_nb += tmp_eval_nb\n",
    "    #     #     nb_eval_examples += label_ids.shape[0]\n",
    "    #     # eval_accuracy = eval_accurate_nb/nb_eval_examples\n",
    "    #     # print(\"Validation Accuracy: {}\".format(eval_accuracy))\n",
    "    #     # scheduler.step(eval_accuracy)\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "        \n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('valid_acc', acc)\n",
    "        self.log('valid_loss', loss, on_step=True)\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        loss = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        logits = self.model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "\n",
    "        acc = self.accurate_nb(logits, b_labels)\n",
    "\n",
    "        self.log('test_acc', acc)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        ###\n",
    "        # Param Optim.\n",
    "        ###\n",
    "        param_optimizer = list(self.named_parameters())\n",
    "        no_decay = ['bias', 'gamma', 'beta']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': self.hparams.weight_decay},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay_rate': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1)\n",
    "        # return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'eval_acc'}\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "    def accurate_nb(self, preds, labels):\n",
    "        pred_flat = torch.argmax(preds, dim=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return torch.sum(pred_flat == labels_flat) / labels_flat.shape[0]\n",
    "\n",
    "def main():\n",
    "    TRAIN_BATCH_SIZE = 512\n",
    "    VAL_BATCH_SIZE = 512\n",
    "    TEST_BATCH_SIZE = 32\n",
    "\n",
    "    LEARNING_RATE = 0.1\n",
    "    EPOCHS = 3\n",
    "    WEIGHT_DECAY = 0.2\n",
    "\n",
    "    SEED = 7\n",
    "    \n",
    "    data = ProjData(max_len=128, ratio=2)\n",
    "    try:\n",
    "        data.load()\n",
    "        print(\"Loaded Saved Data\")\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        data.setup()\n",
    "        data.save()\n",
    "    ### Comment out the try block and uncomment below while you're working on the data part or you'll just skip it and use old data.\n",
    "    # data.setup() \n",
    "    # data.save()\n",
    "    print(\"Defining model\")\n",
    "    model = ProjModel(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"nlp_proj\")\n",
    "\n",
    "    # trainer = pl.Trainer(logger=logger, accelerator='dp', gpus=4, fast_dev_run=False, weights_summary='full', gradient_clip_val=1.0)\n",
    "    # trainer = pl.Trainer(logger=logger, accelerator='dp', gpus=4, gradient_clip_val=1.0, max_epochs=4)\n",
    "    trainer = pl.Trainer(logger=logger, accelerator='dp', gpus=1, gradient_clip_val=1.0, max_epochs=20)\n",
    "    trainer.fit(model, data.train_dataloader(batch_size=TRAIN_BATCH_SIZE), data.val_dataloader(batch_size=VAL_BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Saved Data\n",
      "Defining model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PermissionError' object has no attribute 'message'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 \u001b[0;31m# todo: specify the possible exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDummyExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/base.py\u001b[0m in \u001b[0;36mget_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m_get_file_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_writers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             self.file_writer = FileWriter(self.log_dir, self.max_queue,\n\u001b[0m\u001b[1;32m    251\u001b[0m                                           self.flush_secs, self.filename_suffix)\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         self.event_writer = EventFileWriter(\n\u001b[0m\u001b[1;32m     61\u001b[0m             log_dir, max_queue, flush_secs, filename_suffix)\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         self._file_name = (\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \"\"\"\n\u001b[0;32m--> 669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'tb_logs/nlp_proj/version_1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-d37b4d14a3e9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;31m# trainer = pl.Trainer(logger=logger, accelerator='dp', gpus=4, gradient_clip_val=1.0, max_epochs=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_clip_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVAL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# plugin will setup fitting (e.g. ddp will launch child processes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# dispath `start_training` or `start_testing` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mpre_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# save exp to get started (this is where the first experiment logs are written)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams_initial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_hyperparams\u001b[0;34m(self, params, metrics)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrank_zero_only\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml_proj2/lib/python3.9/site-packages/pytorch_lightning/loggers/tensorboard.py\u001b[0m in \u001b[0;36mlog_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'\\n you tried to log {v} which is not currently supported. Try a dict or a scalar/tensor.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrank_zero_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PermissionError' object has no attribute 'message'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
