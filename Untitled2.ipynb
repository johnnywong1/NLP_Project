{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2d2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: torchvision in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (0.9.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied: numpy in /u/kem5en/.local/lib/python3.6/site-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: torch==1.8.1 in /u/kem5en/.local/lib/python3.6/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: dataclasses in /u/kem5en/.local/lib/python3.6/site-packages (from torch==1.8.1->torchvision) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /u/kem5en/.local/lib/python3.6/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /u/kem5en/.conda/envs/ml_proj/include/python3.6m/UNKNOWN\n",
      "sysconfig: /u/kem5en/.conda/envs/ml_proj/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c23db4e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'UnidentifiedImageError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-edd3575914e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/torchvision/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlsun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSUNClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoCaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCocoDetection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcifar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCIFAR100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstl10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTL10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/torchvision/datasets/lsun.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/u/kem5en/.conda/envs/ml_proj/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# PILLOW_VERSION is deprecated and will be removed in a future release.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Use __version__ instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mImageMode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mTiffTags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'UnidentifiedImageError'"
     ]
    }
   ],
   "source": [
    "# flake8: noqa\n",
    "# yapf: disable\n",
    "\n",
    "# __import_lightning_begin__\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import os\n",
    "# __import_lightning_end__\n",
    "\n",
    "# __import_tune_begin__\n",
    "import shutil\n",
    "import tempfile\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, \\\n",
    "    TuneReportCheckpointCallback\n",
    "# __import_tune_end__\n",
    "\n",
    "\n",
    "# __lightning_begin__\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    This has been adapted from\n",
    "    https://towardsdatascience.com/from-pytorch-to-pytorch-lightning-a-gentle-introduction-b371b7caaf09\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data_dir=None):\n",
    "        super(LightningMNISTClassifier, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir or os.getcwd()\n",
    "\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def accuracy(self, logits, labels):\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        return torch.tensor(accuracy)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in outputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "\n",
    "    @staticmethod\n",
    "    def download_data(data_dir):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "        ])\n",
    "        return MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        mnist_train = self.download_data(self.data_dir)\n",
    "\n",
    "        self.mnist_train, self.mnist_val = random_split(\n",
    "            mnist_train, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=int(self.batch_size))\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=int(self.batch_size))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "def train_mnist(config):\n",
    "    model = LightningMNISTClassifier(config)\n",
    "    trainer = pl.Trainer(max_epochs=10, show_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model)\n",
    "# __lightning_end__\n",
    "\n",
    "\n",
    "# __tune_train_begin__\n",
    "def train_mnist_tune(config, data_dir=None, num_epochs=10, num_gpus=0):\n",
    "    model = LightningMNISTClassifier(config, data_dir)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        # If fractional GPUs passed in, convert to int.\n",
    "        gpus=math.ceil(num_gpus),\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[\n",
    "            TuneReportCallback(\n",
    "                {\n",
    "                    \"loss\": \"ptl/val_loss\",\n",
    "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
    "                },\n",
    "                on=\"validation_end\")\n",
    "        ])\n",
    "    trainer.fit(model)\n",
    "# __tune_train_end__\n",
    "\n",
    "\n",
    "# __tune_train_checkpoint_begin__\n",
    "def train_mnist_tune_checkpoint(config,\n",
    "                                checkpoint_dir=None,\n",
    "                                data_dir=None,\n",
    "                                num_epochs=10,\n",
    "                                num_gpus=0):\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        # If fractional GPUs passed in, convert to int.\n",
    "        gpus=math.ceil(num_gpus),\n",
    "        logger=TensorBoardLogger(\n",
    "            save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        progress_bar_refresh_rate=0,\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                metrics={\n",
    "                    \"loss\": \"ptl/val_loss\",\n",
    "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
    "                },\n",
    "                filename=\"checkpoint\",\n",
    "                on=\"validation_end\")\n",
    "        ])\n",
    "    if checkpoint_dir:\n",
    "        # Currently, this leads to errors:\n",
    "        # model = LightningMNISTClassifier.load_from_checkpoint(\n",
    "        #     os.path.join(checkpoint, \"checkpoint\"))\n",
    "        # Workaround:\n",
    "        ckpt = pl_load(\n",
    "            os.path.join(checkpoint_dir, \"checkpoint\"),\n",
    "            map_location=lambda storage, loc: storage)\n",
    "        model = LightningMNISTClassifier._load_model_state(\n",
    "            ckpt, config=config, data_dir=data_dir)\n",
    "        trainer.current_epoch = ckpt[\"epoch\"]\n",
    "    else:\n",
    "        model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n",
    "\n",
    "    trainer.fit(model)\n",
    "# __tune_train_checkpoint_end__\n",
    "\n",
    "\n",
    "# __tune_asha_begin__\n",
    "def tune_mnist_asha(num_samples=10, num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir = os.path.expanduser(\"~/data\")\n",
    "    LightningMNISTClassifier.download_data(data_dir)\n",
    "\n",
    "    config = {\n",
    "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"batch_size\": tune.choice([32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        max_t=num_epochs,\n",
    "        grace_period=1,\n",
    "        reduction_factor=2)\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_mnist_tune,\n",
    "            data_dir=data_dir,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"tune_mnist_asha\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "# __tune_asha_end__\n",
    "\n",
    "\n",
    "# __tune_pbt_begin__\n",
    "def tune_mnist_pbt(num_samples=10, num_epochs=10, gpus_per_trial=0):\n",
    "    data_dir = os.path.expanduser(\"~/data\")\n",
    "    LightningMNISTClassifier.download_data(data_dir)\n",
    "\n",
    "    config = {\n",
    "        \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "        \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "    }\n",
    "\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        perturbation_interval=4,\n",
    "        hyperparam_mutations={\n",
    "            \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "            \"batch_size\": [32, 64, 128]\n",
    "        })\n",
    "\n",
    "    reporter = CLIReporter(\n",
    "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"])\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_mnist_tune_checkpoint,\n",
    "            data_dir=data_dir,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter,\n",
    "        name=\"tune_mnist_pbt\")\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "\n",
    "# __tune_pbt_end__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "583d1ce6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_mnist_asha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5b627000c428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# ASHA scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtune_mnist_asha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Population based training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtune_mnist_pbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tune_mnist_asha' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.smoke_test:\n",
    "        tune_mnist_asha(num_samples=1, num_epochs=6, gpus_per_trial=0)\n",
    "        tune_mnist_pbt(num_samples=1, num_epochs=6, gpus_per_trial=0)\n",
    "    else:\n",
    "        # ASHA scheduler\n",
    "        tune_mnist_asha(num_samples=10, num_epochs=10, gpus_per_trial=0)\n",
    "        # Population based training\n",
    "        tune_mnist_pbt(num_samples=10, num_epochs=10, gpus_per_trial=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
